Next we list the papers that each member read,
along with their summary and critique.

\subsection{Papers read by Ye Zhou}
The first paper was the Pegasus paper by U Kang
\begin{itemize*}
\item {\em Main idea}: 
      When graph data grows larger and larger, using traditional graph mining algorithms is difficult to deal with trend. In order to solve the graph 
      mining problems with several Petabytes data, Pegasus is the first such library, impelemted on the top of Hadoop platform. In the paper, first they 
      try to find out the common operation, which is the matrix-vector multipilication, underlying several primitive graph mining operations. They call it 
      GIM-V. As GIM-V is so important, they successfully proposed several optimizations, and got more than 5 times faster performance. They also took real 
      big data graph into Pegasus to get the mining result, which revealed important patterns. This showed the succuess of Pegasus with large data graph 
      mininig as the graph data they used had never been studied before.
\item {\em Use for our project}:
      It showed that with large data, we always have new problems using traditional graph mining methods. It is really hard to say that with SQL, we can 
      do enough with graph mining now, as SQL is based on RMDB. But with more and more new mature products/framework like hive, pig, shark which support 
      traditional SQL operations on big data and No-SQL database, we can do more with SQL. 
\item {\em Shortcomings}:
      PEGASUS focus on large graph querying/mining and most of the job was focused on how to compute fast, but it ignored the storage part which can also 
      take effect to improve the performance like indexing. In addition, PEGASUS essentially perform node/vertex-centralized computation but cannot 
      supports edge-centralized processing like induced subgraphs. Finally, hadoop is not so efficient for iterative calculation, as everytime it needs to 
      write output data to hard disk. Spark has better performance as it output its intermediate data in memory and can even cahche the data in memory.
\end{itemize*}

\subsection{Papers read by Ye Zhou}
The second paper was the Spectral Analysis for Billion-Scale Graphs paper by U Kang
\begin{itemize*}
\item {\em Main idea}: 
      The paper proposed HEIGEN algorithm which is designed to be accurate, efficient to run on highly scalable hadoop environment and solve the problems 
      that will calculate out the spectral value. The paper first showed specific observation using HEIGEN with real world data on billion-scale graphs,
      focusing on structural property of networks: spotting near-cliques and finding triangles. Then the author explained that the alternatives for computing
      the eigenvalues of symmetric matrix including Power Method, Simultaneous iteration and Lanczos-NO are not suitable for big data on mapreduce. So the author
      described the algorithm for computing the top K eigenvectors and eigenvalues with four speficic fields improvement: Careful Algorithm Choice, selective 
      parallelization, blocking and skewness exploiting. Finally it turned out that performance improved in both scalabitily and skewed matrix data, compared with 
      HEIGEN-PLAIN.
\item {\em Use for our project}:
      Largely based on mapreduce, HEIGEN is a totally new algorithm. What we can learn is that with massive data, we can change the former way of thinking for graph mining,
      so that we can largely improve the performance without SQL. And also, with the infrastructure of hadoop, and the way map/reduce reading data, we can do modification
      to use the advantages to get even better performance.
\item {\em Shortcomings}:
      Highly based on map/reduce architecture also brings lots of problems that hadoop has. Such as the job schedualing and data shuffling. As the matrix operation needs to
      read large amount of data, and for iterative calculation, spark is a better choice as everything is in memory. 
\end{itemize*}

\subsection{Papers read by Ye Zhou}
The third paper was the Unifying Guil-by-Association Approaches paper by Koutra
\begin{itemize*}
\item {\em Main idea}: 
      The paper mainly proposed FaBP, which is a Fast Belief Propagation algorithm on Hadoop. It first compare and contrast several very successful, guilt-by-association methods:
      Random Walk with Restarts, Semi-Supervised Learning and Belief Propagation. The author showed that these three methods are closedly related but not identical. Then he proposed
      the algorithm FaBP, showed the experiments result. It turned out that the accuracy keeps the same or even better with the traditional BP, but the performance is twice better.
      It also has convergence guarantee. It is even sensitive to the "about-half" homophily factor, as long as the latter is within the convergence bounds. It also scales linearly
      on the number of edges. 
\item {\em Use for our project}:
      Learn the way using hadoop to impelment the algorithm for machine learning. 
\item {\em Shortcomings}:
      Again it is based on Hadoop, the performance for iterative calculation is not so good compared with spark. And BP algorithm is not so efficient when dealing with graph which
      has circle. And the convergence is limited due to specific requirement.
\end{itemize*}

\subsection{Papers read by Jin Hu}
The first paper was the GBASE paper by U Kang
\begin{itemize*}
\item {\em Main idea}: The paper introduces a general graph management system GBase for large scale graph storage and computation.
\item {\em The main contribution of the paper:}:
      GBase uses "compressed block encoding" method to make graph storage more efficiently.
    For graph indexing, the paper succeeds in handling multiple type of queries on a large graph instead of a specific type and is suitable for distributed environment. By supporting homogeneous block level indexing and being flexible in both edge and node centralized computing, GBase has better properties than similar distributed systems.
    The framework the paper proposes also supported both graph-level and node-level queries, making it applicable to various applications.
    GBase partitions data in two dimensions to better use the block and community-like properties of real-world graphs,
    which gives it advantage over either row-oriented or column-oriented storages.
\item {\em Limitations}:
      The paper's indexing method handles large graphs successfully, but its property compared to frequent subgraph
    or significant graph pattern methods are not shown in the experiment. Optional indexing methods may be added to
    the system.
\end{itemize*}

\subsection{Papers read by Jin Hu}
The second paper was by Danai Koutra
\begin{itemize*}
\item {\em Main idea}: The paper does the comparison among some of the most popular guilt-by-association method.
\item {\em The main contribution of the paper:}:
      The paper manages to prove that all methods result in a similar matrix inversion problem. In addition, the paper proposes a fast and accurate BP algorithm. In theory, the paper finds that RWR(Personalized Random Walk with Restats), SSL(Semi-Supervised Learning) and BP(Belief Propagation) are closely related, but not the same. RWR and SSL are not heterophily, but BP is heterophily. All three methods are scalable. RWR and SSL have convergence while BP is unknown. The proposed FABP method has nice property with all these perspectives. FABP is an approximation of standard BP, but FABP is significantly faster based on the experiment and guarantees convergence, which makes it better than BP. The experiments also verify the paper's ideas. The author tested the theory and the properties of the proposed FABP method in terms of accuracy, convergence, sensitivity to parameters and scalability.
\end{itemize*}

\subsection{Papers read by Jin Hu}
The third paper was by Ignacio Alvarez-Hamelin
\begin{itemize*}
\item {\em Main idea}: The paper introduces K-core decomposition and its application in the visulization of large scale networks.
\item {\em The main contribution of the paper:}:
	K-core decomposition can find subgraphs which all of the nodes in the subgraph have degrees higher than k after removing nodes with lower coreness. This method can find the subgraphs which are more closely connected and achieves "clustering" in large graphs.
      As K-core decomposition can produce two-dimensional layout of large scale networks with their important topological and hierarchical properties, the paper takes advantage of the K-core algorithm to allow visulization of network and offer features like fingerprint identification and general analysis assistance. The visualization algorithm has linear running time proportional to the size of the network, making it well scalable for large networks. In addition, the algorithm offers 2D representation of networks which makes information visualization more accessible than other representations and the parameters of the algorithm are universally defined, which makes it suitable for all types of networks.
\item {\em Limitations}:
      The proposed visualization algorithm still utilizes certain parameters to identify the properties of the network, which involves considerable human interactions and prior expemental knowledge. Self adjusting parameters might be a huge improvement and can be an interesting topic to follow.
\end{itemize*}

$\ldots$

